{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import LDA, count vectorizer, and dataset from sklearn.\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Import plotting modules.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# This ensures that the plots show up in the notebook. \n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function will print out the top words for each topic.\n",
    "def print_top_words(lda_model, vocabulary, n_top_words):\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        print \"Topic %d top %d words:\" % (topic_idx, n_top_words)\n",
    "        print \", \".join([vocabulary[i] for i in topic.argsort()[:-n_top_words-1:-1]])\n",
    "        print\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_topic_importance(transformed_corpus):\n",
    "    print  '   '.join([(\"Topic %d\" % t) for t in range(transformed_corpus.shape[1])])\n",
    "    for document_topic_weights in compressed_corpus:\n",
    "        print '      '.join([str(round(document_topic_weights[t], 2)) for t in range(transformed_corpus.shape[1])])\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Analyzing topics in Spam data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...        NaN   \n",
       "8  spam  WINNER!! As a valued network customer you have...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "2        NaN        NaN  \n",
       "5        NaN        NaN  \n",
       "8        NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r\"C:\\Users\\Javier\\Desktop\\Coursera\\Python\\General_Assembly\\Data Sets\\spam.csv\"\n",
    "\n",
    "spam_ham = pd.read_csv(path, encoding= 'latin1')\n",
    "spam_ham = spam_ham[spam_ham.v1 == \"spam\"]\n",
    "spam_ham.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# counting frequency of words inside your corpus\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=2, ngram_range=(1, 1))\n",
    "\n",
    "# here you're fitting your data into the model\n",
    "vectorized_corpus = vectorizer.fit_transform(spam_ham.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model object.\n",
    "lda_model = LatentDirichletAllocation(n_topics=10, learning_method='batch')\n",
    "\n",
    "# Fit the model on the data and transform the data.\n",
    "compressed_corpus = lda_model.fit_transform(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 top 5 words:\n",
      "nokia, ur, week, free, 16\n",
      "\n",
      "Topic 1 top 5 words:\n",
      "prize, min, contact, claim, won\n",
      "\n",
      "Topic 2 top 5 words:\n",
      "prize, won, claim, customer, stop\n",
      "\n",
      "Topic 3 top 5 words:\n",
      "stop, free, ur, mobile, text\n",
      "\n",
      "Topic 4 top 5 words:\n",
      "free, games, ur, special, reveal\n",
      "\n",
      "Topic 5 top 5 words:\n",
      "reply, text, free, video, new\n",
      "\n",
      "Topic 6 top 5 words:\n",
      "free, orange, latest, mins, double\n",
      "\n",
      "Topic 7 top 5 words:\n",
      "free, txt, www, ur, com\n",
      "\n",
      "Topic 8 top 5 words:\n",
      "txt, win, www, 100, entry\n",
      "\n",
      "Topic 9 top 5 words:\n",
      "claim, holiday, cash, award, sae\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most common words within the Spam data set\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "print_top_words(lda_model, vocabulary, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing topics in Non-Spam data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    v1                                                 v2 Unnamed: 2  \\\n",
       "0  ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1  ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "3  ham  U dun say so early hor... U c already then say...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "3        NaN        NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_data = pd.read_csv(path, encoding= 'latin1')\n",
    "ham_data = ham_data[ham_data.v1 == \"ham\"]\n",
    "ham_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# counting frequency of words inside your corpus\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=2, ngram_range=(1, 1))\n",
    "\n",
    "# here you're fitting your data into the model\n",
    "vectorized_corpus = vectorizer.fit_transform(ham_data.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model object.\n",
    "lda_model = LatentDirichletAllocation(n_topics=10, learning_method='batch')\n",
    "\n",
    "# Fit the model on the data and transform the data.\n",
    "compressed_corpus = lda_model.fit_transform(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 top 5 words:\n",
      "later, sorry, ll, gonna, life\n",
      "\n",
      "Topic 1 top 5 words:\n",
      "ok, work, night, doing, right\n",
      "\n",
      "Topic 2 top 5 words:\n",
      "day, good, love, hope, send\n",
      "\n",
      "Topic 3 top 5 words:\n",
      "gt, lt, did, like, got\n",
      "\n",
      "Topic 4 top 5 words:\n",
      "don, know, da, want, going\n",
      "\n",
      "Topic 5 top 5 words:\n",
      "hi, dear, ur, happy, dont\n",
      "\n",
      "Topic 6 top 5 words:\n",
      "just, know, need, dont, pls\n",
      "\n",
      "Topic 7 top 5 words:\n",
      "come, want, text, tomorrow, ll\n",
      "\n",
      "Topic 8 top 5 words:\n",
      "way, just, ì_, come, ìï\n",
      "\n",
      "Topic 9 top 5 words:\n",
      "lor, home, ok, wat, ì_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most common words within the Spam data set\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "print_top_words(lda_model, vocabulary, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeding Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building the train and test sets!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_set = pd.read_csv(path, encoding= 'latin1')\n",
    "dummies = pd.get_dummies(data_set.iloc[:,0])\n",
    "data_set = data_set.join(dummies)\n",
    "\n",
    "x = vectorized_corpus = vectorizer.fit_transform(data_set.iloc[:,1])\n",
    "y = np.array(data_set.iloc[:,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(penalty = 'l2', C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Reg accuracy is 0.98\n",
      "AUC is 0.93\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print \"Logistic Reg accuracy is %2.2f\" % accuracy_score(y_test, model.predict(x_test))\n",
    "print \"AUC is %2.2f\" % roc_auc_score(y_test, model.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031791</td>\n",
       "      <td>0.968209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.029396</td>\n",
       "      <td>0.970604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085015</td>\n",
       "      <td>0.914985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008764</td>\n",
       "      <td>0.991236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.997580</td>\n",
       "      <td>0.002420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.031791  0.968209\n",
       "1  0.029396  0.970604\n",
       "2  0.085015  0.914985\n",
       "3  0.008764  0.991236\n",
       "4  0.997580  0.002420"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.predict_proba(x_test)\n",
    "\n",
    "predictions = pd.DataFrame(a)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = gnb.fit(x.toarray(), y).predict(x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 5572 points : 544\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\"% (x.toarray().shape[0],(y != y_pred).sum()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
